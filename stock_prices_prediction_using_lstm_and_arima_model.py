# -*- coding: utf-8 -*-
"""Stock Prices Prediction Using LSTM and ARIMA model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11W-a0FiYhrcSSsKyq8KPcv_xWytrpeqj
"""

!pip install tensorflow -qqq
!pip install keras -qqq
!pip install yfinance -qqq

import tensorflow as tf
import keras
import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Check TensorFlow version
print("TensorFlow Version: ", tf.__version__)

# Fetch AAPL data
goldman_data = yf.download('GS', start='2020-01-01', end='2024-06-01')

# Display the first few rows of the dataframe
goldman_data.head()

# @title Open

from matplotlib import pyplot as plt
goldman_data['Open'].plot(kind='line', figsize=(8, 4), title='Open')
plt.gca().spines[['top', 'right']].set_visible(False)

# Checking for missing values
goldman_data.isnull().sum()

# Filling missing values, if any
goldman_data.fillna(method='ffill', inplace=True)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0,1))
goldman_data_scaled = scaler.fit_transform(goldman_data['Close'].values.reshape(-1,1))

X = []
y = []

for i in range(60, len(goldman_data_scaled)):
    X.append(goldman_data_scaled[i-60:i, 0])
    y.append(goldman_data_scaled[i, 0])

train_size = int(len(X) * 0.8)
test_size = len(X) - train_size

X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]
X_train, y_train = np.array(X_train), np.array(y_train)
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, AdditiveAttention, Permute, Reshape, Multiply

model = Sequential()

# Adding LSTM layers with return_sequences=True
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(LSTM(units=50, return_sequences=True))

# Adding self-attention mechanism
# The attention mechanism
attention = AdditiveAttention(name='attention_weight')
# Permute and reshape for compatibility
model.add(Permute((2, 1)))
model.add(Reshape((-1, X_train.shape[1])))
attention_result = attention([model.output, model.output])
multiply_layer = Multiply()([model.output, attention_result])
# Return to original shape
model.add(Permute((2, 1)))
model.add(Reshape((-1, 50)))

# Adding a Flatten layer before the final Dense layer
model.add(tf.keras.layers.Flatten())

# Final Dense layer
model.add(Dense(1))

from keras.layers import BatchNormalization

# Adding Dropout and Batch Normalization
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.compile(optimizer='adam', loss='mean_squared_error')

model.summary()

# Assuming X_train and y_train are already defined and preprocessed
history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2)
from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=10)
history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2, callbacks=[early_stopping])

from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, CSVLogger

# Callback to save the model periodically
model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')

# Callback to reduce learning rate when a metric has stopped improving
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)

# Callback for TensorBoard
tensorboard = TensorBoard(log_dir='./logs')

# Callback to log details to a CSV file
csv_logger = CSVLogger('training_log.csv')

# Combining all callbacks
callbacks_list = [early_stopping, model_checkpoint, reduce_lr, tensorboard, csv_logger]

# Fit the model with the callbacks
history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2, callbacks=callbacks_list)

# Convert X_test and y_test to Numpy arrays if they are not already
X_test = np.array(X_test)
y_test = np.array(y_test)

# Ensure X_test is reshaped similarly to how X_train was reshaped
# This depends on how you preprocessed the training data
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# Now evaluate the model on the test data
test_loss = model.evaluate(X_test, y_test)
print("Test Loss: ", test_loss)

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Making predictions
y_pred = model.predict(X_test)

# Calculating MAE and RMSE
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)

print("Mean Absolute Error: ", mae)
print("Root Mean Square Error: ", rmse)

import yfinance as yf
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Fetch the latest 60 days of AAPL stock data
data = yf.download('GS', period='3mo', interval='1d')

# Select 'Close' price and scale it
closing_prices = data['Close'].values.reshape(-1, 1)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(closing_prices)

# Predict the next 4 days iteratively
predicted_prices = []
current_batch = scaled_data[-60:].reshape(1, 60, 1)  # Most recent 60 days

for i in range(4):  # Predicting 4 days
    # Get the prediction (next day)
    next_prediction = model.predict(current_batch)

    # Reshape the prediction to fit the batch dimension
    next_prediction_reshaped = next_prediction.reshape(1, 1, 1)

    # Append the prediction to the batch used for predicting
    current_batch = np.append(current_batch[:, 1:, :], next_prediction_reshaped, axis=1)

    # Inverse transform the prediction to the original price scale
    predicted_prices.append(scaler.inverse_transform(next_prediction)[0, 0])

print("Predicted Stock Prices for the next 4 days: ", predicted_prices)

predicted_prices

!pip install mplfinance -qqq
import pandas as pd
import mplfinance as mpf
import matplotlib.dates as mpl_dates
import matplotlib.pyplot as plt

# Assuming 'data' is your DataFrame with the fetched AAPL stock data
# Make sure it contains Open, High, Low, Close, and Volume columns

# Creating a list of dates for the predictions
last_date = data.index[-1]
next_day = last_date + pd.Timedelta(days=1)
prediction_dates = pd.date_range(start=next_day, periods=4)

# Assuming 'predicted_prices' is your list of predicted prices for the next 4 days
predictions_df = pd.DataFrame(index=prediction_dates, data=predicted_prices, columns=['Close'])
print(predictions_df)
# Plotting the actual data with mplfinance
mpf.plot(data, type='candle', style='charles', volume=True)

# Overlaying the predicted data
plt.figure(figsize=(10,6))
plt.plot(predictions_df.index, predictions_df['Close'], linestyle='dashed', marker='o', color='red')

plt.title("Goldman Sachs Stock Price with Predicted Next 4 Days")
plt.show()

predictions_df

"""# Final Visual for prediction:"""

import pandas as pd
import mplfinance as mpf
import matplotlib.dates as mpl_dates
import matplotlib.pyplot as plt

# Fetch the latest 60 days of AAPL stock data
data = yf.download('GS', period='3mo', interval='1d') # Fetch 64 days to display last 60 days in the chart

# Select 'Close' price and scale it
closing_prices = data['Close'].values.reshape(-1, 1)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(closing_prices)

# Predict the next 4 days iteratively
predicted_prices = []
current_batch = scaled_data[-60:].reshape(1, 60, 1)  # Most recent 60 days

for i in range(4):  # Predicting 4 days
    next_prediction = model.predict(current_batch)
    next_prediction_reshaped = next_prediction.reshape(1, 1, 1)
    current_batch = np.append(current_batch[:, 1:, :], next_prediction_reshaped, axis=1)
    predicted_prices.append(scaler.inverse_transform(next_prediction)[0, 0])

# Creating a list of dates for the predictions
last_date = data.index[-1]
next_day = last_date + pd.Timedelta(days=1)
prediction_dates = pd.date_range(start=next_day, periods=4)

# Adding predictions to the DataFrame
predicted_data = pd.DataFrame(index=prediction_dates, data=predicted_prices, columns=['Close'])

# Combining both actual and predicted data
combined_data = pd.concat([data['Close'], predicted_data['Close']])
combined_data = combined_data[-64:] # Last 60 days of actual data + 4 days of predictions

# Plotting the actual data
plt.figure(figsize=(10,6))
plt.plot(data.index[-60:], data['Close'][-60:], linestyle='-', marker='o', color='blue', label='Actual Data')

# Plotting the predicted data
plt.plot(prediction_dates, predicted_prices, linestyle='-', marker='o', color='red', label='Predicted Data')

plt.title("Goldman Sachs Stock Price: Last 60 Days and Next 4 Days Predicted")
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

"""# Prediction for any date:"""

import yfinance as yf
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime, timedelta

def predict_stock_price(input_date):
    # Check if the input date is a valid date format
    try:
        input_date = pd.to_datetime(input_date)
    except ValueError:
        print("Invalid Date Format. Please enter date in YYYY-MM-DD format.")
        return

    # Fetch data from yfinance
    end_date = input_date
    start_date = input_date - timedelta(days=90)  # Fetch more days to ensure we have 60 trading days
    data = yf.download('GS', start=start_date, end=end_date)

    if len(data) < 60:
        print("Not enough historical data to make a prediction. Try an earlier date.")
        return

    # Prepare the data
    closing_prices = data['Close'].values[-60:]  # Last 60 days
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(closing_prices.reshape(-1, 1))

    # Make predictions
    predicted_prices = []
    current_batch = scaled_data.reshape(1, 60, 1)

    for i in range(4):  # Predicting 4 days
        next_prediction = model.predict(current_batch)
        next_prediction_reshaped = next_prediction.reshape(1, 1, 1)
        current_batch = np.append(current_batch[:, 1:, :], next_prediction_reshaped, axis=1)
        predicted_prices.append(scaler.inverse_transform(next_prediction)[0, 0])

    # Output the predictions
    for i, price in enumerate(predicted_prices, 1):
        print(f"Day {i} prediction: {price}")

# Example use
user_input = input("Enter a date (YYYY-MM-DD) to predict Goldman Sachs stock for the next 4 days: ")
predict_stock_price(user_input)

"""# Build and Train ARIMA Model"""

import yfinance as yf

# Download data
gs = yf.download("GS", start="2011-01-01", end="2024-07-01")

import pandas as pd
# Preprocess data
dataset_ex_df = gs.copy()
dataset_ex_df = dataset_ex_df.reset_index()
dataset_ex_df['Date'] = pd.to_datetime(dataset_ex_df['Date'])
dataset_ex_df.set_index('Date', inplace=True)
dataset_ex_df = dataset_ex_df['Close'].to_frame()

!pip install pmdarima

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf
from statsmodels.tsa.arima.model import ARIMA
from pmdarima import auto_arima


# Auto ARIMA to select optimal ARIMA parameters
model = auto_arima(dataset_ex_df['Close'], seasonal=False, trace=True)
print(model.summary())

from statsmodels.tsa.arima.model import ARIMA
import numpy as np

# Define the ARIMA model
def arima_forecast(history):
    # Fit the model
    model = ARIMA(history, order=(0,1,0))
    model_fit = model.fit()

    # Make the prediction
    output = model_fit.forecast()
    yhat = output[0]
    return yhat

# Split data into train and test sets
X = dataset_ex_df.values
size = int(len(X) * 0.8)
train, test = X[0:size], X[size:len(X)]

# Walk-forward validation
history = [x for x in train]
predictions = list()
for t in range(len(test)):
    # Generate a prediction
    yhat = arima_forecast(history)
    predictions.append(yhat)
    # Add the predicted value to the training set
    obs = test[t]
    history.append(obs)

predictions

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6), dpi=100)
plt.plot(dataset_ex_df.iloc[size:,:].index, test, label='Real')
plt.plot(dataset_ex_df.iloc[size:,:].index, predictions, color='red', label='Predicted')
plt.title('ARIMA Predictions vs Actual Values')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

predictions

# Fetch stock data
df = yf.download('GS', start='2011-01-01', end='2024-07-01')
df = df[['Close']]
# Prepare data for LSTM
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(df)

def create_dataset(data, lookback):
    X, y = [], []
    for i in range(len(data) - lookback - 1):
        X.append(data[i:(i + lookback), 0])
        y.append(data[i + lookback, 0])
    return np.array(X), np.array(y)

lookback = 60
X, y = create_dataset(scaled_data, lookback)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# Split data into training and testing sets
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Build LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(lookback, 1)))
model.add(LSTM(50))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

# Train LSTM model
model.fit(X_train, y_train, epochs=100, batch_size=40, validation_data=(X_test, y_test))

# Make LSTM predictions
lstmpredictions = model.predict(X_test)
lstmpredictions = scaler.inverse_transform(lstmpredictions)

lstmpredictions

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Assuming predictions (ARIMA predictions) and y_pred (LSTM predictions) are already defined
# Ensure both predictions have the same length
min_length = min(len(predictions), len(lstmpredictions), len(test))

# Adjust the lengths of all arrays to the minimum length
predictions = np.array(predictions[:min_length])
y_pred = np.array(lstmpredictions[:min_length])
test = np.array(test[:min_length])

# Assuming `test_index` holds the indices corresponding to the test set
test_index = df.index[train_size:train_size + min_length]

# Create a DataFrame to hold the predictions
predictions_ens = pd.DataFrame({
    'ARIMA': predictions,
    'LSTM': y_pred.flatten() # Ensure y_pred is flattened if it's in a different shape
})

# Ensemble method: simple average of ARIMA and LSTM predictions
predictions_ens['Ensemble'] = predictions_ens.mean(axis=1)

predictions_ens

# Ensemble method: simple average of ARIMA and LSTM predictions
predictions_ens['Ensemble'] = predictions_ens.mean(axis=1)

# Plot the results
plt.figure(figsize=(12, 6), dpi=100)
plt.plot(test_index, test, label='Real')
plt.plot(test_index, predictions_ens['ARIMA'], label='ARIMA Predictions', linestyle='--')
plt.plot(test_index, predictions_ens['LSTM'], label='LSTM Predictions', linestyle='--')
plt.plot(test_index, predictions_ens['Ensemble'], color='red', label='Ensemble Predictions')
plt.title('ARIMA, LSTM, and Ensemble Predictions vs Actual Values')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

"""# BACKTEST LOGIC"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from statsmodels.tsa.arima.model import ARIMA
from pmdarima import auto_arima
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from math import sqrt

# Fetch data
symbol = 'GS'  # You can change this to any stock symbol
start_date = '2010-01-01'
end_date = '2023-07-28'  # Update this to a recent date

data = yf.download(symbol, start=start_date, end=end_date)

# Split the data into training and testing sets
train_size = int(len(data) * 0.8)
train_data = data[:train_size]
test_data = data[train_size:]

# ARIMA Model
# Find the best ARIMA parameters
arima_model = auto_arima(train_data['Close'], start_p=1, start_q=1,
                         test='adf', max_p=3, max_q=3, m=1,
                         d=None, seasonal=False, start_P=0,
                         D=0, trace=True, error_action='ignore',
                         suppress_warnings=True, stepwise=True)

# Fit the ARIMA model
arima_model_fit = arima_model.fit(train_data['Close'])

# Make predictions
arima_predictions = arima_model_fit.predict(n_periods=len(test_data))

print("Data preparation and model training complete.")

# Preprocess data for LSTM
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset_ex_df)

train_data_scaled = scaled_data[:train_size]
test_data_scaled = scaled_data[train_size:]

X_train, y_train = [], []
for i in range(60, len(train_data_scaled)):
    X_train.append(train_data_scaled[i-60:i, 0])
    y_train.append(train_data_scaled[i, 0])

X_train, y_train = np.array(X_train), np.array(y_train)
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

# Build and train LSTM model
lstm_model = Sequential()
lstm_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
lstm_model.add(LSTM(units=50, return_sequences=False))
lstm_model.add(Dense(units=1))

lstm_model.compile(optimizer='adam', loss='mean_squared_error')
lstm_model.fit(X_train, y_train, epochs=5, batch_size=32)

# Generate LSTM predictions
lstms_predictions = []
for i in range(60, len(test_data_scaled)):
    X_test = test_data_scaled[i-60:i, 0]
    X_test = np.reshape(X_test, (1, X_test.shape[0], 1))
    predicted_price = lstm_model.predict(X_test)
    lstms_predictions.append(scaler.inverse_transform(predicted_price)[0][0])

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from math import sqrt

class BacktestingFramework:
    def __init__(self, data, initial_capital=100000, transaction_cost=0.001):
        self.data = data
        self.initial_capital = initial_capital
        self.transaction_cost = transaction_cost
        self.reset()

    def reset(self):
        self.capital = self.initial_capital
        self.shares = 0
        self.trades = []
        self.portfolio_values = [self.initial_capital]

    def execute_trade(self, action, price, date):
        if action == 'buy' and self.capital > 0:
            shares_to_buy = self.capital // price
            cost = shares_to_buy * price * (1 + self.transaction_cost)
            if cost <= self.capital:
                self.shares += shares_to_buy
                self.capital -= cost
                self.trades.append(('buy', date, shares_to_buy, price))
        elif action == 'sell' and self.shares > 0:
            revenue = self.shares * price * (1 - self.transaction_cost)
            self.capital += revenue
            self.trades.append(('sell', date, self.shares, price))
            self.shares = 0

    def calculate_portfolio_value(self, price):
        return self.capital + self.shares * price

    def backtest(self, model_type, predictions):
        self.reset()

        print(f"Shape of test data: {self.data.shape}")
        print(f"Length of predictions: {len(predictions)}")
        print(f"First few prices: {self.data['Close'].values[:5]}")
        print(f"First few predictions: {predictions[:5]}")

        # Ensure predictions is a numpy array
        predictions = np.array(predictions)

        # Ensure we only iterate over the available predictions
        max_iterations = min(len(self.data), len(predictions))

        for i in range(1, max_iterations):
            try:
                current_price = self.data['Close'].iloc[i]
                prev_price = self.data['Close'].iloc[i-1]
                date = self.data.index[i]

                prediction = predictions[i-1]

                if prediction > current_price:
                    self.execute_trade('buy', current_price, date)
                elif prediction < current_price:
                    self.execute_trade('sell', current_price, date)

                portfolio_value = self.calculate_portfolio_value(current_price)
                self.portfolio_values.append(portfolio_value)
            except Exception as e:
                print(f"Error at iteration {i}: {str(e)}")
                print(f"Current price: {current_price}")
                print(f"Previous price: {prev_price}")
                print(f"Date: {date}")
                print(f"Prediction: {prediction}")
                raise

        return self.calculate_performance_metrics()

    def calculate_performance_metrics(self):
        total_return = (self.portfolio_values[-1] - self.initial_capital) / self.initial_capital
        sharpe_ratio = np.mean(np.diff(self.portfolio_values)) / np.std(np.diff(self.portfolio_values)) * sqrt(252)
        max_drawdown = np.min(np.minimum.accumulate(self.portfolio_values) / np.maximum.accumulate(self.portfolio_values) - 1)

        return {
            'Total Return': total_return,
            'Sharpe Ratio': sharpe_ratio,
            'Max Drawdown': max_drawdown,
            'Number of Trades': len(self.trades)
        }

    def plot_results(self, model_type):
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
        ax1.plot(self.data.index, self.data['Close'], label='Actual Price')
        ax1.plot(self.data.index[:len(self.portfolio_values)], self.portfolio_values, label='Portfolio Value')
        ax1.set_title(f'{model_type} Model - Stock Price vs Portfolio Value')
        ax1.legend()

        for trade in self.trades:
            if trade[0] == 'buy':
                ax1.scatter(trade[1], trade[3], color='g', marker='^')
            else:
                ax1.scatter(trade[1], trade[3], color='r', marker='v')

        ax2.plot(self.data.index[:len(self.portfolio_values)], self.portfolio_values)
        ax2.set_title('Portfolio Value Over Time')
        plt.tight_layout()
        plt.show()

import numpy as np
import pandas as pd

# Data checks
print("Test array shape:", test.shape)
print("\nFirst few elements of test array:")
print(test[:5])
print("\nShape of predictions:", predictions.shape)
print("First few predictions:", predictions[:5])

# Ensure test and predictions have compatible lengths
if len(predictions) > len(test):
    predictions = predictions[:len(test)]
elif len(predictions) < len(test):
    test = test[:len(predictions)]

# Flatten the 2D test array to 1D
test_flattened = test.flatten()

# Create a simple DataFrame from the flattened test array
test_df = pd.DataFrame({'Close': test_flattened}, index=pd.date_range(start='2023-01-01', periods=len(test_flattened), freq='D'))

# Instantiate the BacktestingFramework
backtest = BacktestingFramework(test_df)

print("\nARIMA Model Results:")
arima_results = backtest.backtest('ARIMA', predictions)
print(arima_results)
backtest.plot_results('ARIMA')
#print("\nLSTM Model Results:")
#lstm_results = backtest.backtest('LSTM', lstmpredictions)
#print(lstm_results)
#backtest.plot_results('LSTM')